{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load MLP_Skeleton.py\n",
    "\"\"\"\n",
    "SANAD SAHA\n",
    "933 620 612\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "try:\n",
    "   import _pickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a class for a LinearTransform layer which takes an input \n",
    "# weight matrix W and computes W x as the forward step\n",
    "class LinearTransform(object):\n",
    "\n",
    "    def __init__(self, W, b):\n",
    "    # DEFINE __init function\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "    def forward(self, x):\n",
    "    # DEFINE forward function\n",
    "        Z = np.dot(x, self.W) + self.b\n",
    "        return Z\n",
    "    \n",
    "        \n",
    "        \n",
    "    def backward(\n",
    "        self, \n",
    "        grad_output, \n",
    "        learning_rate=0.0, \n",
    "        momentum=0.0, \n",
    "        l2_penalty=0.0,\n",
    "    ):\n",
    "    # DEFINE backward function\n",
    "        print(\"Backward not completed\")\n",
    "# ADD other operations in LinearTransform if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a class for a ReLU layer max(x,0)\n",
    "class ReLU(object):\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x[x < 0.0] = 0.0\n",
    "        x = np.maximum(x, 0)\n",
    "        return x\n",
    "\n",
    "    def backward(\n",
    "        self, \n",
    "        grad_output, \n",
    "        learning_rate=0.0, \n",
    "        momentum=0.0, \n",
    "        l2_penalty=0.0,\n",
    "    ):\n",
    "    # DEFINE backward function\n",
    "        print(\"relu backprop not done\")\n",
    "# ADD other operations in ReLU if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a class for a sigmoid layer followed by a cross entropy layer, the reason \n",
    "# this is put into a single layer is because it has a simple gradient form\n",
    "class SigmoidCrossEntropy(object):\n",
    "    def forward(self, x):\n",
    "        self.x = 1 / (1 + np.exp(-x))\n",
    "        return self.x\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate=0.0, momentum=0.0, l2_penalty=0.0):\n",
    "        print(\"Nothing\")\n",
    "        # DEFINE backward function\n",
    "    \n",
    "# ADD other operations and data entries in SigmoidCrossEntropy if needed\n",
    "    def compute_cost(self, y, target_y):\n",
    "        m = target_y.shape[1]\n",
    "        logprobs = np.multiply(np.log(y),target_y)/m +  np.multiply(np.log(1 - y), 1- target_y)/m\n",
    "        cost = - np.sum(logprobs) \n",
    "        cost = np.squeeze(cost)\n",
    "        return cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.6\n",
      "63.9\n",
      "64.2\n",
      "66.3\n",
      "68.2\n",
      "69.3\n",
      "70.7\n",
      "69.1\n",
      "70.1\n",
      "70.6\n",
      "71.39999999999999\n",
      "71.89999999999999\n",
      "72.39999999999999\n",
      "73.2\n",
      "73.6\n",
      "75.0\n",
      "74.4\n",
      "75.1\n",
      "75.9\n",
      "75.7\n"
     ]
    }
   ],
   "source": [
    "# This is a class for the Multilayer perceptron\n",
    "class MLP(object):\n",
    "\n",
    "    def __init__(self, input_dims, hidden_units):\n",
    "    # INSERT CODE for initializing the network\n",
    "        self.hidden_units = hidden_units\n",
    "        self.input_dims = input_dims\n",
    "        #initialize W1, w2, b1, b2\n",
    "        \n",
    "        self.w1 = np.random.randn(self.input_dims, self.hidden_units) * np.sqrt(2/(self.input_dims - 1))\n",
    "        self.b1 = np.zeros((1, self.hidden_units))\n",
    "        self.w2 = np.random.randn(self.hidden_units, 1) * np.sqrt(2/(self.hidden_units - 1))\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "\n",
    "#         assert (self.w1.shape == (self.hidden_units, self.input_dims))\n",
    "#         assert (self.b1.shape == (self.hidden_units, 1))\n",
    "#         assert (self.w2.shape == (1, self.hidden_units))\n",
    "#         assert (self.b2.shape == (1, 1))\n",
    "        \n",
    "    def train(self, x_batch, y_batch, learning_rate = 0.0, momentum = 0.0, l2_penalty = 0.0):\n",
    "    # INSERT CODE for training the network\n",
    "        m = x_batch.shape[0]\n",
    "        \n",
    "        lt1 = LinearTransform(self.w1, self.b1)\n",
    "        z1 = lt1.forward(x_batch)\n",
    "#         sc = SigmoidCrossEntropy()\n",
    "#         a1 = sc.forward(z1)\n",
    "        r = ReLU()\n",
    "#         print(z1[:3, :4])\n",
    "        a1 = r.forward(z1)\n",
    "#         print(a1.shape)\n",
    "#         print(a1[:3, :4])\n",
    "        #print(x_batch[1:10, 1:10])\n",
    "        #print(self.w1[3, 1:3])\n",
    "        \n",
    "        \n",
    "        lt2 = LinearTransform(self.w2, self.b2)\n",
    "        z2 = lt2.forward(a1)\n",
    "#         print(z2.shape)\n",
    "#         print(z2[:10, :])\n",
    "        sigmoid_cross_entropy = SigmoidCrossEntropy()\n",
    "        a2 = sigmoid_cross_entropy.forward(z2)\n",
    "#         print(a2.shape)\n",
    "#         print(a2[:10, :])\n",
    "        #self.eeval(a2, y_batch)\n",
    "        \n",
    "        #loss = sigmoid_cross_entropy.compute_cost(a2, y_batch)\n",
    "        #print(\"loss \")\n",
    "        #print(loss)\n",
    "        #BACKPROP\n",
    "        assert(a2.shape == y_batch.shape)\n",
    "        dz2 = a2 - y_batch\n",
    "        dw2 = np.dot(a1.T, dz2)\n",
    "        db2 = np.sum(dz2, axis = 0, keepdims = True)\n",
    "        dw2 /= m\n",
    "        db2 /= m\n",
    "        #print(db2)\n",
    "        \n",
    "        assert (self.w2.shape == dw2.shape) \n",
    "        assert (self.b2.shape == db2.shape)\n",
    "        \n",
    "        grad_relu = (z1 > 0.0)\n",
    "        \n",
    "        vera = np.dot(dz2, self.w2.T)\n",
    "        dz1 = np.multiply(vera, grad_relu)\n",
    "        dw1 = np.dot(x_batch.T, dz1)\n",
    "        db1 = np.sum(dz1, axis = 0, keepdims = True)\n",
    "        dw1 /= m\n",
    "        db1 /= m\n",
    "        assert(dw1.shape == self.w1.shape)\n",
    "        assert (self.b1.shape == db1.shape)\n",
    "       \n",
    "        #print(a2.shape)\n",
    "        #print(self.evaluate(a2, y_batch))\n",
    "        \n",
    "        #print(\"Testing\")\n",
    "        \n",
    "#         print(self.w1[10, :3])\n",
    "        \n",
    "        self.w1 = self.w1 - learning_rate * dw1\n",
    "        self.b1 = self.b1 - learning_rate * db1\n",
    "        self.w2 = self.w2 - learning_rate * dw2\n",
    "        self.b2 = self.b2 - learning_rate * db2\n",
    "        \n",
    "        return self.eeval(a2, y_batch)\n",
    "#         print(self.w1[10, :3])\n",
    "        #print(dw2)\n",
    "        #print(\"Match testing \")\n",
    "        #print(self.w1[10, 1:5])\n",
    "            \n",
    "    def eeval(self, y, ycap):\n",
    "        t = (y > 0.4999999)\n",
    "#         print(ycap[:5,:])\n",
    "#         print(t[:5,:])\n",
    "        pp = np.count_nonzero(t==ycap)\n",
    "        return(float(pp)/float(y.shape[0])*100)\n",
    "#         missmatch = np.sum()\n",
    "#         print(dhon[:5, :])\n",
    "#         print(y.shape[0])\n",
    "#         print(missmatch)\n",
    "#         print(float(missmatch)/float(y.shape[0])*100)\n",
    "        \n",
    "    def evaluate(self, x, y):\n",
    "        \n",
    "        lt1 = LinearTransform(self.w1, self.b1)\n",
    "        z1 = lt1.forward(x)\n",
    "        r = ReLU()\n",
    "        a1 = r.forward(z1)\n",
    "        lt2 = LinearTransform(self.w2, self.b2)\n",
    "        z2 = lt2.forward(a1)\n",
    "        sigmoid_cross_entropy = SigmoidCrossEntropy()\n",
    "        a2 = sigmoid_cross_entropy.forward(z2)\n",
    "        predictions = (a2 > 0.5)\n",
    "        accuracy = float((np.dot(predictions.T, y) + np.dot(1-predictions.T, 1-y))/float(y.size)*100)\n",
    "        return accuracy\n",
    "    \n",
    "    # INSERT CODE for testing the network\n",
    "# ADD other operations and data entries in MLP if needed\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if sys.version_info[0] < 3:\n",
    "        data = pickle.load(open('cifar_2class_py2.p', 'rb'))\n",
    "    else:\n",
    "        data = pickle.load(open('cifar_2class_py2.p', 'rb'), encoding='bytes')\n",
    "    \n",
    "    train_x = data[b'train_data']\n",
    "    train_y = data[b'train_labels']\n",
    "    test_x = data[b'test_data']\n",
    "    test_y = data[b'test_labels']\n",
    "\n",
    "    num_examples, input_dims = train_x.shape\n",
    "    #print(train_x.shape)\n",
    "    #normalizing the input\n",
    "    x_norm = np.linalg.norm(train_x, ord = 2, axis = 0, keepdims = True)\n",
    "    \n",
    "    # Divide x by its norm.\n",
    "    train_x = train_x/x_norm\n",
    "#     train_x = train_x - train_x.mean()\n",
    "#     train_x = train_x / train_x.var()\n",
    "#     print(train_x[0, :4])\n",
    "#     print(train_y[:10,:])\n",
    "#     print(train_x.shape)\n",
    "#     print(train_y.shape)\n",
    "#     print (train_x)\n",
    "    \n",
    "    # INSERT YOUR CODE HERE\n",
    "    # YOU CAN CHANGE num_epochs AND num_batches TO YOUR DESIRED VALUES\n",
    "    num_epochs = 10\n",
    "    num_batches = 1000\n",
    "    hidden_units = 50\n",
    "    \n",
    "    mlp = MLP(input_dims, hidden_units)\n",
    "    for i in range (6000):\n",
    "        \n",
    "        b = mlp.train(train_x[:1000, :], train_y[:1000, :], 0.3)\n",
    "        \n",
    "        if(i % 300 == 0):\n",
    "            print(b)\n",
    "    \n",
    "  \n",
    "    \n",
    "      \n",
    "#     frm = epoch * num_batches\n",
    "#     to = (epoch+1) * num_batches \n",
    "#     mlp.train(train_x[frm:to, :].T, train_y[frm:to, :].T, 1.5)\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "  \n",
    "#     #INSERT YOUR CODE FOR EACH EPOCH HERE\n",
    "        \n",
    "    \n",
    "#         #train_accuracy = evaluate()\n",
    "#         for b in range(num_batches):\n",
    "#             total_loss = 0.0\n",
    "#             # INSERT YOUR CODE FOR EACH MINI_BATCH HERE\n",
    "#             mlp.train(train_x[b * 10: (b+1) * 10, :], train_y[b * 10: (b+1) * 10, :], 0.005)\n",
    "#             # MAKE SURE TO UPDATE total_loss\n",
    "# #             print(\n",
    "# #                 '\\r[Epoch {}, mb {}]    Avg.Loss = {:.3f}'.format(\n",
    "# #                     epoch + 1,\n",
    "# #                     b + 1,\n",
    "# #                     total_loss,\n",
    "# #                 ),\n",
    "# #                 end='',\n",
    "# #             )\n",
    "# #             sys.stdout.flush()\n",
    "#         train_accuracy = evaluate(train_x, train_y)\n",
    "        # INSERT YOUR CODE AFTER ALL MINI_BATCHES HERE\n",
    "        # MAKE SURE TO COMPUTE train_loss, train_accuracy, test_loss, test_accuracy\n",
    "#         print()\n",
    "#         print('    Train Loss: {:.3f}    Train Acc.: {:.2f}%'.format(\n",
    "#             train_loss,\n",
    "#             100. * train_accuracy,\n",
    "#         ))\n",
    "#         print('    Test Loss:  {:.3f}    Test Acc.:  {:.2f}%'.format(\n",
    "#             test_loss,\n",
    "#             100. * test_accuracy,\n",
    "#         ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test(object):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "    def change(self, p):\n",
    "        self.x = p  \n",
    "        \n",
    "    def show(self):\n",
    "        print(self.x)\n",
    "        \n",
    "    def add(self, y):\n",
    "        self.x = self.x + y;\n",
    "        return self.x;\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00019625  0.01292163 -0.00461242  0.01164016 -0.00482791]\n",
      " [-0.0033735   0.02425993  0.0082251  -0.00367694  0.0042211 ]\n",
      " [ 0.00021651 -0.02202308  0.00196516 -0.00436201 -0.01342334]\n",
      " [-0.00373149  0.0047105  -0.00043725 -0.00994808  0.01175125]\n",
      " [ 0.00875227 -0.00092618 -0.00659876 -0.01794115  0.00930897]]\n",
      "[[0.00019625 0.01292163 0.         0.01164016 0.        ]\n",
      " [0.         0.02425993 0.0082251  0.         0.0042211 ]\n",
      " [0.00021651 0.         0.00196516 0.         0.        ]\n",
      " [0.         0.0047105  0.         0.         0.01175125]\n",
      " [0.00875227 0.         0.         0.         0.00930897]]\n"
     ]
    }
   ],
   "source": [
    "# t = test(5)\n",
    "# t.show()\n",
    "# t.change(20)\n",
    "# t.show()\n",
    "# t2 = test(100)\n",
    "# t2.show()\n",
    "# t2.change(-10)\n",
    "# t2.show()\n",
    "# t.change(10)\n",
    "# print(t2.add(50))\n",
    "# print(t.add(20))\n",
    "# t.show()\n",
    "pp = np.random.randn(5,5) * 0.01\n",
    "print(pp)\n",
    "pp = np.maximum(pp, 0)\n",
    "print(pp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
